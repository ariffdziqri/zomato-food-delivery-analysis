{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cda67-4f6a-4436-9dd5-8d19f110c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad3fce-594f-4be8-8a2f-e03ad1931597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/df.csv\")\n",
    "df = df.drop(columns=[\"Festival\", \"Type_of_order\", \"Type_of_vehicle\"])\n",
    "categorical_cols = [\"Weather_conditions\",\n",
    "                    \"Road_traffic_density\",\n",
    "                    \"City\"]\n",
    "\n",
    "df_wide = pd.get_dummies(df, columns=categorical_cols, drop_first=False) # form wide for hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f737c-a527-40ff-aa5e-ac56de6a9f1f",
   "metadata": {},
   "source": [
    "### Experimenting Models and Hyperparameter Tuning\n",
    "#### We decided to run individually instead of looping because its faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e978f-9a90-41c4-8cd0-ce7bd0f8fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_wide[\"Time_taken (min)\"]\n",
    "X = df_wide.drop(columns=[\"Time_taken (min)\", \"ID\", \"Time_Orderd\", \n",
    "                            \"Time_Order_picked\", \"Order_Date\", 'Restaurant_latitude', \n",
    "                           'Restaurant_longitude', 'Delivery_location_latitude', \n",
    "                           'Delivery_location_longitude', 'Unnamed: 0', \"Delivery_person_Ratings\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size =0.1, random_state = 0)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea2ba5-d8dd-4b65-8e78-9031e45d9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = {\n",
    "    \"model\":[\"Knn Regressor\", \"GradientBoosting Regressor\", \"RandomForest Regressor\", \"ID3 Regressor\"],\n",
    "    \"best_params\": [],\n",
    "    \"valid_accuracy\":[],\n",
    "    \"train_MSE\": [],\n",
    "    \"valid_MSE\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03a653-b2fb-4008-9183-744fbd833d55",
   "metadata": {},
   "source": [
    "### Knn Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619df58-53cb-48be-a2fe-1d1d0c69d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(metric='euclidean')\n",
    "param_grid = {\"n_neighbors\":[1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,              # your pipeline\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',  # or 'neg_root_mean_squared_error'\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3. Fit on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# append to dict for final report\n",
    "report_dict[\"best_params\"].append(grid_search.best_params_)\n",
    "report_dict[\"valid_accuracy\"].append(round(best_model.score(X_valid, y_valid),3))\n",
    "report_dict[\"train_MSE\"].append(round(mean_squared_error(y_train, y_train_pred),3))\n",
    "report_dict[\"valid_MSE\"].append(round(mean_squared_error(y_valid, y_valid_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f2bd4-f99e-4f45-9221-3fff6d9b54b6",
   "metadata": {},
   "source": [
    "### GradientBoosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957d6b0-17b5-46a9-b493-b0f311bb44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "    'max_depth': [5, 7, 10],  # Maximum depth of the individual regression estimators\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb,              # your pipeline\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',  # or 'neg_root_mean_squared_error'\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# append to dict for final report\n",
    "report_dict[\"best_params\"].append(grid_search.best_params_)\n",
    "report_dict[\"valid_accuracy\"].append(round(best_model.score(X_valid, y_valid),3))\n",
    "report_dict[\"train_MSE\"].append(round(mean_squared_error(y_train, y_train_pred),3))\n",
    "report_dict[\"valid_MSE\"].append(round(mean_squared_error(y_valid, y_valid_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b5191-af83-4160-a0f7-17cf88d7d31a",
   "metadata": {},
   "source": [
    "### RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16693827-fc49-4b95-a212-a58e8d761068",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "    'max_depth': [5, 7, 10],  # Maximum depth of the individual regression estimators\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,              # your pipeline\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',  # or 'neg_root_mean_squared_error'\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# append to dict for final report\n",
    "report_dict[\"best_params\"].append(grid_search.best_params_)\n",
    "report_dict[\"valid_accuracy\"].append(round(best_model.score(X_valid, y_valid),3))\n",
    "report_dict[\"train_MSE\"].append(round(mean_squared_error(y_train, y_train_pred),3))\n",
    "report_dict[\"valid_MSE\"].append(round(mean_squared_error(y_valid, y_valid_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45025710-9142-48c4-9820-f90cd77156f3",
   "metadata": {},
   "source": [
    "### ID3 Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9fdf6-3c50-4dbc-a874-4f957d24be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id3 = DecisionTreeRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [5, 7, 10],  # Maximum depth of the individual regression estimators\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=id3,              # your pipeline\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',  # or 'neg_root_mean_squared_error'\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "\n",
    "# append to dict for final report\n",
    "report_dict[\"best_params\"].append(grid_search.best_params_)\n",
    "report_dict[\"valid_accuracy\"].append(round(best_model.score(X_valid, y_valid),3))\n",
    "report_dict[\"train_MSE\"].append(round(mean_squared_error(y_train, y_train_pred),3))\n",
    "report_dict[\"valid_MSE\"].append(round(mean_squared_error(y_valid, y_valid_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633f70d-5e17-4f66-a860-353417390ee7",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cd6f5-2931-40a5-b802-ac07eb1a9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report1 = pd.DataFrame(report_dict)\n",
    "report1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742b714-71eb-4d82-a30e-3edc2bc059eb",
   "metadata": {},
   "source": [
    "#### According to the table, GradientBoost is our best model. However, let's try stacking the models to compensate weakness each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f76ef-a889-4a4b-b80a-f13ac94a7b53",
   "metadata": {},
   "source": [
    "### Stacking Meta-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753b11b-e8e0-4e34-8508-525aa9e32f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Delivery_person_Age', 'Vehicle_condition', 'multiple_deliveries', \n",
    "               'Weather_conditions', 'Road_traffic_density', 'City', \"distance_miles\"]\n",
    "numeric_features = ['Delivery_person_Age', 'Vehicle_condition', 'multiple_deliveries', \"distance_miles\"]\n",
    "categorical_features = ['Weather_conditions', 'Road_traffic_density', 'City']\n",
    "\n",
    "estimators = [\n",
    "    ('GradientBooosting', GradientBoostingRegressor(max_depth = 7, n_estimators = 100, random_state = 0)),\n",
    "    ('DecisionTree', DecisionTreeRegressor(max_depth=10, random_state=0)),\n",
    "    ('Knn', KNeighborsRegressor(metric='euclidean', n_neighbors=6)),\n",
    "]\n",
    "\n",
    "X = df[all_features]\n",
    "y = df['Time_taken (min)']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, random_state = 0, test_size=0.1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, random_state = 0, test_size=0.5)\n",
    "\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop=None), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', StackingRegressor(\n",
    "    estimators=estimators, final_estimator=RandomForestRegressor(max_depth=10, n_estimators=100, random_state=0)))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ffe71-d8ce-4a1b-98bd-8bad9c0877b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "\n",
    "# append to dict for final report\n",
    "report_dict[\"model\"].append(\"Stacking Meta-Model\")\n",
    "report_dict[\"valid_accuracy\"].append(round(clf.score(X_valid, y_valid),3))\n",
    "report_dict[\"best_params\"].append(None)\n",
    "report_dict[\"train_MSE\"].append(round(mean_squared_error(y_train, y_train_pred),3))\n",
    "report_dict[\"valid_MSE\"].append(round(mean_squared_error(y_valid, y_valid_pred),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff898ffa-5037-4671-a347-a3694393f6b9",
   "metadata": {},
   "source": [
    "### Output Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e33fe-914b-4c9c-b9b6-d6abb1c9c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(report_dict)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f2e8e-1cae-4ecf-88ec-2508e6b77b41",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb27e1a-cf1d-4cce-8ae1-835d1b6e7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_wide[\"Time_taken (min)\"]\n",
    "X = df_wide.drop(columns=[\"Time_taken (min)\", \"ID\", \"Time_Orderd\", \n",
    "                            \"Time_Order_picked\", \"Order_Date\", 'Restaurant_latitude', \n",
    "                           'Restaurant_longitude', 'Delivery_location_latitude', \n",
    "                           'Delivery_location_longitude', 'Unnamed: 0', \"Delivery_person_Ratings\"])\n",
    "\n",
    "estimators = [\n",
    "    ('GradientBooosting', GradientBoostingRegressor(max_depth = 7, n_estimators = 100)),\n",
    "    ('DecisionTree', DecisionTreeRegressor(max_depth=10, random_state=0)),\n",
    "    ('Knn', KNeighborsRegressor(metric='euclidean', n_neighbors=6))\n",
    "]\n",
    "clf = StackingRegressor(\n",
    "    estimators=estimators, final_estimator=RandomForestRegressor(max_depth=10, n_estimators=100)\n",
    ")\n",
    "\n",
    "clf.fit(X, y)\n",
    "pi = permutation_importance(estimator=clf, X=X, y=y, random_state=0)\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\" : pi.importances_mean\n",
    "})\n",
    "\n",
    "importances = importances.sort_values(\"importance\", ascending=False)\n",
    "plt.barh(importances[\"feature\"], importances[\"importance\"])\n",
    "plt.xticks(rotation='vertical') \n",
    "plt.tight_layout()\n",
    "plt.title('Feature importance using Stacking')\n",
    "_ = plt.ylabel(r'reduction in $R^2$ on shuffling feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1db818-210e-4861-8807-637d1970a5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
